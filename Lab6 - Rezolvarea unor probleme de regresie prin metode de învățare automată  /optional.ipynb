{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ce se întamplă în cazul clasificarii binare daca se modifică pragul de decizie din 0.5 în alte valori. Cum se poate aprecia calitatea clasificatorului pentru diferite valori ale pragului?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raspuns: Daca pragul este mai mare (de exemplu 0.8) atunci sansele ca algoritmul sa prezica corect label-ul pozitiv sunt mai mici. Aceasta situatie poate conduce de exemplu la clasificarea unor pacienti bolnavi ca fiind sanatosi, ceea ce nu este de dorit. Daca pragul este mai mic, sansele ca cei bolnavi sa fie clasificati corect sunt mai mari. (nu este atat de grav daca pacientii sanatosi sunt considerati bolnavi) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rezolvarea unei probleme de regresie/clasificare prin: folosirea validarii încrucișate (K-fold cross validation) - pb cu happiness dupa pib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "from sklearn import linear_model\n",
    "import pandas as pd \n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "#Ce îi poate face pe oameni fericiți? - dupa PIB\n",
    "def readData(dataPath: str):\n",
    "    df = pd.read_csv(dataPath, delimiter=',', header='infer')\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "#split data frame in k sets\n",
    "def splitDataInKSets(dataFrame, k):\n",
    "    size = dataFrame.shape[0]\n",
    "    arr = np.array_split(range(size),k) \n",
    "    input = [[dataFrame[\"Economy..GDP.per.Capita.\"].iloc[i] for i in index] for index in arr]\n",
    "    output = [[dataFrame[\"Happiness.Score\"].iloc[i] for i in index] for index in arr]\n",
    "    return input, output\n",
    "\n",
    "def getErrors(computed_output, validation_output):\n",
    "    computedError = mean_squared_error(validation_output, computed_output)\n",
    "    return computedError\n",
    "\n",
    "def trainRegressor(regressor, dataFrame, k):\n",
    "    errors = []\n",
    "    input, output = splitDataInKSets(dataFrame, k)\n",
    "    for i in range(0, k):\n",
    "        validationInputSet = input[i]\n",
    "        validationOutputSet = output[i]\n",
    "        trainingInputSet = []\n",
    "        trainingOutputSet = []\n",
    "        for j in range(0, k):\n",
    "            if j != i:\n",
    "                trainingInputSet += input[j]\n",
    "                trainingOutputSet += output[j]\n",
    "        regressor.partial_fit([[trainingInputSet[ind]] for ind in range(0, len(trainingInputSet))], trainingOutputSet)\n",
    "        computed_output = regressor.predict([[validationInputSet[ind]] for ind in range(0, len(validationInputSet))])\n",
    "        errors.append(getErrors(computed_output, validationOutputSet))\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.179377752289922, 2.28892688827479, 0.4669109843562096, 0.5233600573819017, 0.5543750269744534, 0.4012853667884819]\n",
      "Overall error =  2.402372679344293\n"
     ]
    }
   ],
   "source": [
    "dataFrame = readData(\"2017.csv\")\n",
    "regressor = linear_model.SGDRegressor()\n",
    "errors = trainRegressor(regressor, dataFrame, 6)\n",
    "print(errors)\n",
    "overallError = sum(errors) / len(errors)\n",
    "print(\"Overall error = \", overallError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigarea diferitelor funcții de loss - pt pb 1 - PIB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Squared Error** (squared_error)\n",
    "    - The ordinary least squares is the square of the difference between the actual value and predicted value.\n",
    "    - It tends to penalize model more and more for larger differences thereby giving more weight to outliers\n",
    "\n",
    "- **Huber** (huber)\n",
    "    - The mean squared error (MSE) or squared error gives too much importance to outliers and Mean Average error (MAE) (here instead of squaring we take absolute value of errors) gives equal weightage to all points\n",
    "    - Huber loss combines MSE and MAE to give best of both wold- it is quadratic(MSE) when the error is small else MAE\n",
    "\n",
    "- **Epsilon Insensitive** (epsilon_insensitive)\n",
    "    - The value of epsilon determines the distance within which errors are considered to be zero . The loss function ignores error which are less than or equal to epsilon value by treating them zero.\n",
    "    - Thus the loss function effectively forces the optimizer to find such a hyperplane that a tube of width epsilon around this hyperplane will contain all the datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss type:  squared_error  Error =  0.47318649695354564\n",
      "Loss type:  huber  Error =  0.24942280176284015\n",
      "Loss type:  epsilon_insensitive  Error =  0.4844170281281762\n",
      "Loss type:  squared_epsilon_insensitive  Error =  0.5031070050134103\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def getTrainingAndValidationSets(dfWorldHappiness):\n",
    "    dataSize = dfWorldHappiness.shape[0]\n",
    "    \n",
    "    trainingIndexSet = np.random.choice(range(dataSize), size=int(0.8 * dataSize), replace=False)\n",
    "    validationIndexSet = [i for i in range(dataSize) if i not in trainingIndexSet]\n",
    "\n",
    "    trainingInputSet = [dfWorldHappiness[\"Economy..GDP.per.Capita.\"].iloc[index] for index in trainingIndexSet]\n",
    "    trainingOutputSet = [dfWorldHappiness[\"Happiness.Score\"].iloc[index] for index in trainingIndexSet]\n",
    "\n",
    "    validationInputSet = [dfWorldHappiness[\"Economy..GDP.per.Capita.\"].iloc[index] for index in validationIndexSet]\n",
    "    validationOutputSet = [dfWorldHappiness[\"Happiness.Score\"].iloc[index] for index in validationIndexSet]\n",
    "\n",
    "    return trainingInputSet, trainingOutputSet, validationInputSet, validationOutputSet\n",
    "\n",
    "def getRegressor(dataFrame, loss_type):\n",
    "    trainingInputSet, trainingOutputSet, _, _ = getTrainingAndValidationSets(dataFrame)\n",
    "    X = [[el] for el in trainingInputSet]\n",
    "    regressor = linear_model.SGDRegressor(loss=loss_type)\n",
    "    regressor.fit(X, trainingOutputSet)\n",
    "    return regressor\n",
    "\n",
    "def main():\n",
    "    dataFrame = readData(\"2017.csv\")\n",
    "    _,_,validationInput,validationOutput = getTrainingAndValidationSets(dataFrame)\n",
    "    for loss_type in linear_model.SGDRegressor().loss_functions:\n",
    "        regressor = getRegressor(dataFrame, loss_type)\n",
    "        computedOutput = regressor.predict([[validationInput[i]] for i in range(0, len(validationInput))]) \n",
    "        err = metrics.r2_score(validationOutput, computedOutput)\n",
    "        print(\"Loss type: \", loss_type, \" Error = \", err)\n",
    "\n",
    "main()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
